TASKS
==================================================================

Impl: Variable partition blowup, with userdef cap on # variables
    Partition holes into variable names too, to discover constraints between them? Give it a try.
        It might fix cons example, but this is just a tease of later idea
    Test out with pruning all children if no interesting pruning occurred, cap=3
    There aren't quite B_n partitions, since there can't be any new variables on the RHS (not yet considering things like [])

Test generation: Pos/negexs of usages given types

Get things really end to end

Bash script that clears results with flag and automatically runs code and ./d

Side try: Spyro style to compare

Side try: Contexts from unmerged/pointy types - just leaves of indiv fn trees? try a couple more things there


NEXT THINGS TO THINK ABOUT
==================================================================

Assigning variables
    Instead of searching all partitions, start with constraints on what *needs* to be different/shared? vs Unclear?
    Which things vary together? How do we find dependent/independent parameters/variables
    If we have dependencies before even doing search, then when we're enumerating we have way less stuff to pick from
    Dependence graph between parameters gives structure for which variables are shared
    We don't actually care about shape of type stuff, just what is or isn't allowed to vary togheter

    One idea: Instead of partitions, think left to right
    Is this parameter constrained by a previous one, or concrete type, or unconstrained/new variable?
        If we decide it's a label, we might expand the number of holes to fill in rightwards
        Decide if we write down labels and stuff. There's dependencies between functions, but primitives sort of matter?
    Idea: First make function skeleton, then fill in

    Label names: similar, but instead of edges within one function's parameters, across diff functions too


IDEAS FOR LATER
==================================================================

It takes more examples to distinguish esoteric cases (e.g. lots of variables)
    They only have so much expressivity
    Analyze examples to see - how much are we even capable of distinguishing/what is even reasonable to consider given these exs?

Some other way of weighing candidates other than just checking negexs?

Dataflow idea. We also have negative edges which do not flow from negexs.

Clever underapproximations of inhabitation


PAST NOTES
==================================================================

The below examples perform better because they distinguish 0,[]i from true,[]b by erroneously giving any
int-related values one label and bool-related values a different one. The restrictions made by type variables need to
be more first-class
When we do sibling merging, also pick variables
    blowup factor = num partitions of (num var nodes) = nth Bell number

Contexts: 11664
Filter- passes all positives: 130
Some of our favorite candidates:
{0={ℓ1 of []}, cons=(v(a)) -> ({ℓ0 of [_]}) -> {ℓ0 of [_]}, tr={ℓ2 of []}, []i={ℓ0 of [_]}, []b={ℓ0 of [_]}, [[]]i={ℓ0 of [_]}}
Rejects 1 negexs
{0={ℓ2 of []}, cons=(v(a)) -> ({ℓ0 of [_]}) -> {ℓ0 of [_]}, tr={ℓ1 of []}, []i={ℓ0 of [_]}, []b={ℓ0 of [_]}, [[]]i={ℓ0 of [_]}}
Rejects 1 negexs
Total negexs: 5
Max rejected: 3

Candidates which reject the max number of examples:
{0={ℓ0 of [_]}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ1 of []}, []i={ℓ0 of [_]}, []b={ℓ1 of []}, [[]]i={ℓ0 of [_]}}
{0={ℓ0 of [_]}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ2 of []}, []i={ℓ0 of [_]}, []b={ℓ2 of []}, [[]]i={ℓ0 of [_]}}
{0={ℓ0 of [_]}, cons=(v(a)) -> (v(a)) -> {ℓ0 of [_]}, tr={ℓ1 of []}, []i={ℓ0 of [_]}, []b={ℓ1 of []}, [[]]i={ℓ0 of [_]}}
{0={ℓ0 of [_]}, cons=(v(a)) -> (v(a)) -> {ℓ0 of [_]}, tr={ℓ2 of []}, []i={ℓ0 of [_]}, []b={ℓ2 of []}, [[]]i={ℓ0 of [_]}}
{0={ℓ1 of []}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ0 of [_]}, []i={ℓ1 of []}, []b={ℓ0 of [_]}, [[]]i={ℓ1 of []}}
{0={ℓ1 of []}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ2 of []}, []i={ℓ1 of []}, []b={ℓ2 of []}, [[]]i={ℓ1 of []}}
{0={ℓ1 of []}, cons=(v(a)) -> (v(a)) -> {ℓ1 of []}, tr={ℓ0 of [_]}, []i={ℓ1 of []}, []b={ℓ0 of [_]}, [[]]i={ℓ1 of []}}
{0={ℓ1 of []}, cons=(v(a)) -> (v(a)) -> {ℓ1 of []}, tr={ℓ2 of []}, []i={ℓ1 of []}, []b={ℓ2 of []}, [[]]i={ℓ1 of []}}
{0={ℓ2 of []}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ0 of [_]}, []i={ℓ2 of []}, []b={ℓ0 of [_]}, [[]]i={ℓ2 of []}}
{0={ℓ2 of []}, cons=(v(a)) -> (v(a)) -> v(a), tr={ℓ1 of []}, []i={ℓ2 of []}, []b={ℓ1 of []}, [[]]i={ℓ2 of []}}
{0={ℓ2 of []}, cons=(v(a)) -> (v(a)) -> {ℓ2 of []}, tr={ℓ0 of [_]}, []i={ℓ2 of []}, []b={ℓ0 of [_]}, [[]]i={ℓ2 of []}}
{0={ℓ2 of []}, cons=(v(a)) -> (v(a)) -> {ℓ2 of []}, tr={ℓ1 of []}, []i={ℓ2 of []}, []b={ℓ1 of []}, [[]]i={ℓ2 of []}}

Var thing not surprising, soln will inform how we learn labels. Names need to be more first class - which things vary together/related parameters
I really liked how equivalence classes of variables were directly implied by constraints in constraint gen.
Variable bindings don't become interesting until you look at the full type, but there's so many full types and so many possible bindings/partitions

Should we make explicit type lambdas - no. HM

Interleaving sibling child steps more frequently - if every fill step is interleaved with merge, that's the same as just enumerating every variable right away top down. which is kinda what we do with labels anyway

Armando says when synthesizing, we have techniques like enumeration, pruning, or more specialized algorithms, like ones
 designed for numerical constants. Identifying places where we can call out to specialized algos is helpful
Here: figuring out these partitions of variables - find right skeleton and then call subroutine for variable partitions

Debruijin indices - we don't need bc we just assign unique variables, and all lambdas are at the front in HM
