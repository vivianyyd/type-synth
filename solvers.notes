Eager ADT SMT - Amar Shah, Sanjit Seshia 2023:

Eager approach (Barrett et al. 2021) 

It's not obvious why the eager approach is faster on SMTCOMP: Is it bc there's just less overhead bc no need for multiple calls
It's about equal with cvc5 ADT lazy solver

Lazy SMT architecture, i.e., they use a theory specific solver to handle the data types and a core solver to handle the logical formula (Sebastiani 2007).

A common theory solver will use a combination of congruence closure, syntactic unification, and acyclicality checks (Barrett, Shikanian, and Tinelli 2007; Oppen 1980; Reynolds and Blanchette 2017; Reynolds et al. 2018). 
- CVC5, SMTInterpol, Z3 do this
Barret 2007 - Add support for multiple constructors to Oppen by delaying choice of constructors before constructing graph
^ closest comparison
confused by top of page 5 https://theory.stanford.edu/~barrett/pubs/BST07-PDPAR.pdf 
it's not clear why it's better to introduce constructors to replace selectors. if selectors are unique for each constructor it also makes it easy to disambiguate, and that doesn't seem hard

Oppen 1980 - OG algo but only one constructor => no selectors
Reynolds and Blanchette 2017 - codatatypes (i.e. infinite applications of constructors allowed)
Reynolds et al. 2018 - ADTs with shared selectors e.g. fn that takes multiple variants and does something

The scope of our work is quantifier-free ADT queries. 


Modern theory-specific solvers only handle conjunctions of literals/predicates they understand? Or is that just this Oppen 80 paper 

We also care about disjunctions

Synthesizing data structures (not their transformations..)
Can we generalize - make this also be synthesis where we are synthesizing ASTs


sketch's one-hot encoding is not normal 
need unknown ints but adding ints is hard so onehot
when one hot encoding, need to enforce that only one option is true
adds additional constrs

bitvector solvers both lazy eager <- z3 eager (most)
    intermediate data structures to aggressively simplify before feed to sat solver
int sovlers are generally lazy. nonlinear is really slow the moment you multiply two things


From Oppen 1980 https://dl.acm.org/doi/pdf/10.1145/322203.322204, provers that handle recursive DS: 
    Boyer and Moore's prover for recursively defined functions [1]
    Guttag and Musser's prover for ADTs [3]
    Stanford simplifier [8,9] <- on combining decision procedures

Read or not super relevant:

Work on ADTs with quantifiers:

ADT Constrained Horn Clauses (CHCs) - De Angelis et al. (2020) and Kostyukov, Mordvinov, and Fedyukovich (2021)

ADTs with (FOLD) Abstractions - Suter, Dotta, and Kuncak 2010; Pham and Whalen 2014 - restricted forms of recursive functions (called catamorphisms) via partial evaluation
e.g. abstractions: set of elements in tree, free vars in lambda expr


Quantified ADTs: Kovacs, Robillard, and Voronkov (2017)



the theory solver is the one making the additional constrs?
to make it eager you could ask the theory solver to make all the constraints upfront and that would fully characterize everything that goes on in the solver <- eager when it works it's good but if unbounded you need to add artiifcial bounds. ex. arrays really expensive despite bounded. 

lazy approach: produce set of constraits with strictly greater set of satisfying assignments, get sat model, test if ok on our actual constr, if not then add constraints to get closer to real problem
if smaller problem unsat we know the whole thing unsat
be clever about when you instantiate constraints


during synth step all inputs are fixed/concrete bc we're doing cegis
